# MNIST Classification Project

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A comprehensive implementation and comparison of five different machine learning approaches for handwritten digit classification on the MNIST dataset.

## ğŸ“‹ Overview

This project implements and compares classical machine learning and deep learning approaches:

1. **K-Nearest Neighbors (KNN)** - NumPy only implementation
2. **NaÃ¯ve Bayes** - NumPy only with binary features
3. **Linear Classifier** - Both NumPy and PyTorch implementations
4. **Multilayer Perceptron (MLP)** - PyTorch implementation
5. **Convolutional Neural Network (CNN)** - PyTorch implementation

## ğŸ¯ Results

| Method | Test Accuracy | Parameters | Training Time |
|--------|---------------|------------|---------------|
| **CNN** | **99.13%** | ~200K | ~10 min |
| **MLP** | **98.04%** | ~200K | ~5 min |
| **KNN (k=3)** | **93.50%** | N/A | Instant |
| **Linear (PyTorch)** | **92.36%** | 7,850 | ~2 min |
| **Linear (NumPy)** | 86.29% | 7,850 | ~3 min |
| **NaÃ¯ve Bayes** | 83.80% | 7,840 | ~1 min |

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/JakobeAllen/mnist-project.git
cd mnist-project

# Install dependencies
pip install -r requirements.txt
```

### Running the Project

**Option 1: Fast Version (Recommended)** âš¡
```bash
# Download and prepare MNIST dataset
python fast_mnist.py

# Run all experiments
python main_fast.py
```

**Option 2: Original Version**
```bash
# Download MNIST as image files
python easy_download.py

# Run all experiments
python main.py
```

**Option 3: Quick Demo (No Download Required)**
```bash
# Test with synthetic data
python demo.py
```

## ğŸ“ Project Structure

```
mnist-project/
â”œâ”€â”€ knn.py                  # K-Nearest Neighbors (NumPy only)
â”œâ”€â”€ naive_bayes.py          # NaÃ¯ve Bayes classifier (NumPy only)
â”œâ”€â”€ linear_classifier.py    # Linear classifier (NumPy/PyTorch)
â”œâ”€â”€ mlp.py                  # Multilayer Perceptron (PyTorch)
â”œâ”€â”€ cnn.py                  # Convolutional Neural Network (PyTorch)
â”œâ”€â”€ utils.py                # Evaluation and visualization utilities
â”œâ”€â”€ data_loader.py          # Data loading and preprocessing
â”œâ”€â”€ main.py                 # Main experiment runner
â”œâ”€â”€ main_fast.py            # Fast version with optimized data loading
â”œâ”€â”€ fast_mnist.py           # Create fast-loading dataset
â”œâ”€â”€ easy_download.py        # Download MNIST using PyTorch
â”œâ”€â”€ demo.py                 # Quick demo with synthetic data
â””â”€â”€ requirements.txt        # Project dependencies
```

## ğŸ”¬ Implementation Details

### K-Nearest Neighbors
- Pure NumPy implementation (no scikit-learn)
- Euclidean distance metric
- Tested with k=1, 3, 5

### NaÃ¯ve Bayes
- Pure NumPy implementation
- Binary features (threshold=0.5)
- Laplace smoothing

### Linear Classifier
- Two implementations: NumPy (manual gradients) and PyTorch (autograd)
- Softmax activation + Cross-entropy loss
- Gradient descent optimization

### Multilayer Perceptron
- Architecture: 784 â†’ 256 â†’ 128 â†’ 10
- ReLU activation, Dropout regularization
- Adam optimizer

### Convolutional Neural Network
- Architecture: Conv(1â†’32) â†’ MaxPool â†’ Conv(32â†’64) â†’ MaxPool â†’ FC(128) â†’ FC(10)
- ReLU activation, Dropout regularization
- Achieves 99.13% accuracy

## ğŸ“Š Output Files

The project generates:
- `results_summary.json` - All accuracy results
- `method_comparison.png` - Performance comparison chart
- Confusion matrices for each method
- Weight visualizations
- Probability maps

## ğŸ› ï¸ Requirements

- Python 3.8+
- NumPy â‰¥ 1.21.0
- PyTorch â‰¥ 2.0.0
- torchvision â‰¥ 0.15.0
- matplotlib â‰¥ 3.5.0
- scikit-learn â‰¥ 1.0.0 (for metrics only)
- Pillow â‰¥ 9.0.0
- seaborn â‰¥ 0.11.0
- tqdm â‰¥ 4.62.0

## ğŸ“ˆ Key Findings

1. **Deep learning dominates**: CNN (99.13%) and MLP (98.04%) significantly outperform traditional methods
2. **Spatial features matter**: CNN > MLP demonstrates the importance of convolutional layers
3. **Optimization matters**: PyTorch Linear (92.36%) > NumPy Linear (86.29%)
4. **Independence assumption fails**: NaÃ¯ve Bayes (83.80%) struggles with correlated pixels
5. **KNN sweet spot**: k=3 provides the best bias-variance tradeoff

## ğŸ“ Report

See `REPORT_TEMPLATE.md` for the complete project report template.

## ğŸ¤ Contributing

This is an academic project. Feel free to fork and experiment!

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- MNIST dataset by Yann LeCun
- Pattern Recognition course project
- PyTorch and NumPy communities

## ğŸ‘¤ Author

**Jakobe Allen**
- GitHub: [@JakobeAllen](https://github.com/JakobeAllen)

---

**Note**: This project was created as part of a Pattern Recognition course assignment. The MNIST dataset is not included in the repository due to size constraints. Use the provided download scripts to obtain the data.